steps:
  # --- Étape 1: Validation Terraform ---
  - name: 'hashicorp/terraform:1.3.7'
    id: 'Terraform Validate'
    entrypoint: 'sh'
    args:
      - '-c'
      - |
        echo "Validating Terraform configuration..."
        cd terraform
        terraform init -backend-config=bucket=${_TF_STATE_BUCKET}
        terraform validate
        echo "Terraform validation successful."

  # --- Étape 2: Setup Python Env, Install Java & Dependencies (MODIFIÉE) ---
  - name: 'python:3.11'
    id: 'Setup Environment and Dependencies'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Installing Java (OpenJDK 11)..."
        apt-get update -qq && apt-get install -y openjdk-11-jdk --no-install-recommends
        export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
        echo "JAVA_HOME set to $JAVA_HOME" # <-- Changer ici
        echo "Creating virtual environment in /workspace/venv..."
        python -m venv /workspace/venv
        echo "Installing Python dependencies into venv..."
        /workspace/venv/bin/pip install --upgrade pip
        /workspace/venv/bin/pip install \
          -r src/cloud_function_ingest/requirements.txt \
          -r src/spark_job/requirements.txt \
          -r requirements-dev.txt
        echo "Dependencies installed."
        # Exporter JAVA_HOME pour les étapes suivantes
        echo "export JAVA_HOME=$JAVA_HOME" > /workspace/java_env.sh # 


  # --- Étape 3: Linting du code Python (Cloud Function) ---
  - name: 'python:3.11'
    id: 'Lint Cloud Function Code'
    entrypoint: '/workspace/venv/bin/flake8'
    args: [ '--ignore=E501', 'src/cloud_function_ingest/' ]

  # --- Étape 4: Tests Unitaires Python (Cloud Function) ---
  - name: 'python:3.11'
    id: 'Run Cloud Function Unit Tests'
    entrypoint: '/workspace/venv/bin/pytest'
    args: ['src/cloud_function_ingest/tests/']

  - name: 'python:3.11'
    id: 'Run Spark Unit Tests'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        source /workspace/java_env.sh || echo "java_env.sh not found, assuming JAVA_HOME is set."
        echo "Running PySpark tests with JAVA_HOME=$JAVA_HOME..." # 
        # Utiliser le pytest du venv
        /workspace/venv/bin/pytest src/spark_job/tests/


  # --- Étapes Futures (Déploiement...) ---

# Options globales pour le build
options:
  logging: CLOUD_LOGGING_ONLY

# Configuration du timeout global 
# timeout: "1800s" # 30 minutes (l'installation Java/Spark peut prendre du temps)