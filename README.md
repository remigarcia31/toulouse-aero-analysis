# ‚úàÔ∏è Analyse de Donn√©es de Vol ADS-B sur GCP (R√©gion Toulousaine) üá´üá∑

**Un projet Data Engineering End-to-End d√©montrant la construction d'un pipeline de donn√©es robuste, scalable et automatis√© sur Google Cloud Platform pour l'analyse du trafic a√©rien.**

---

## üéØ Objectif du Projet

Ce projet vise √† ing√©rer, traiter, stocker, orchestrer et visualiser les donn√©es publiques de suivi de vols ADS-B (Automatic Dependent Surveillance‚ÄìBroadcast) fournies par OpenSky Network, en se concentrant potentiellement sur la r√©gion de Toulouse, hub a√©ronautique majeur.

L'objectif n'est pas seulement technique, mais aussi de d√©montrer comment un pipeline data moderne sur GCP peut transformer des donn√©es brutes en insights actionnables, **r√©duisant ainsi le temps d'analyse** pour les parties prenantes (op√©rations a√©roportuaires, √©tudes environnementales, etc.) et fournissant une **source de donn√©es fiable et √† jour** sur l'activit√© a√©rienne locale.

Ce repository sert de portfolio pour illustrer mes comp√©tences en Data Engineering sur l'√©cosyst√®me GCP.

## ‚ú® Fonctionnalit√©s et Points Cl√©s

* **Pipeline de Donn√©es E2E :** De l'ingestion brute √† la visualisation, en passant par le traitement et le stockage.
* **Architecture GCP Moderne :** Utilisation de services manag√©s et serverless pour la scalabilit√© et l'efficacit√© op√©rationnelle (Cloud Functions, Cloud Storage, BigQuery, Dataproc, Cloud Composer, Cloud Build).
* **Traitement de Donn√©es Scalable :** Utilisation d'Apache Spark (via PySpark sur Dataproc) pour traiter et transformer les donn√©es JSON brutes en format Parquet optimis√© et partitionn√© (style Hive).
* **Data Warehousing Analytique :** Exposition des donn√©es trait√©es dans BigQuery via une table externe pour des requ√™tes SQL performantes, avec des vues pr√©-calculant des KPIs.
* **Orchestration Automatis√©e :** Utilisation de Cloud Composer (Apache Airflow) pour planifier et g√©rer l'ex√©cution du pipeline de traitement Spark.
* **Infrastructure as Code (IaC) :** D√©finition de l'infrastructure GCP (Buckets, Dataset BQ, IAM...) via Terraform pour la reproductibilit√© et la gestion versionn√©e.
* **CI/CD (Fondations) :** Mise en place de Cloud Build pour l'int√©gration continue (linting, tests unitaires) du code applicatif et la validation de l'infrastructure.
* **Visualisation :** Pr√©paration des donn√©es pour une restitution via un dashboard interactif (Looker Studio). *(Note: Le dashboard lui-m√™me n'est pas dans ce repo, mais la pr√©paration des donn√©es l'est).*

## üèóÔ∏è Architecture

Le diagramme ci-dessous illustre le flux de donn√©es et les composants principaux du projet :

```mermaid
graph LR
    subgraph " "
        direction LR
        subgraph "Ingestion (Temps R√©el-Like)"
            direction TB
            OSN[("OpenSky Network API")] -- JSON --> CF(Cloud Function<br/> / Python)
            SCHED(Cloud Scheduler<br/>Toutes les 10 min) --> CF
            CF -- Fichiers JSON bruts --> GCS_L(GCS Landing<br/>YYYY/MM/DD/HH/)
        end

        subgraph "Traitement (Batch Horaire)"
            direction TB
            COMP(Cloud Composer<br/>Airflow DAG<br/>@hourly) -- D√©clenche Job --> DP(Dataproc Cluster<br/>PySpark Script)
            DP -- Lit depuis --> GCS_L
            DP -- √âcrit Parquet partitionn√© --> GCS_P(GCS Processed<br/>/year=.../month=.../...)
        end

        subgraph "Stockage & Acc√®s"
            direction TB
            GCS_P -- Pointe vers --> BQ_EXT(BigQuery<br/>External Table<br/>flight_data_external)
            BQ_EXT -- Utilis√©e par --> BQ_VIEW(BigQuery Views<br/>vw_kpi_1, vw_kpi_2...)
        end

        subgraph "Consommation"
            direction TB
            BQ_VIEW -- Requ√™tes SQL / API --> USERS(Analystes / Applications)
            BQ_VIEW -- Connecteur BQ --> LOOKER(Looker Studio<br/>Dashboard)
        end

        subgraph "Gestion & CI/CD"
            direction RL
            GIT(Git Repository<br/>Code Source<br/>Terraform, Python, SQL, DAG) -- Push --> CB(Cloud Build<br/>CI: Lint, Test<br/>CD: Deploy Infra/Code)
            TF(Terraform<br/>IaC) --> GCP_RES(Ressources GCP<br/>GCS, BQ, IAM...)
            CB -- D√©ploie --> GCP_RES
            CB -- D√©ploie --> CF
            CB -- D√©ploie --> COMP(DAGs)
        end
    end

    style GCS_L fill:#4285F4,stroke:#000,color:#fff
    style GCS_P fill:#4285F4,stroke:#000,color:#fff
    style CF fill:#DB4437,stroke:#000,color:#fff
    style SCHED fill:#DB4437,stroke:#000,color:#fff
    style DP fill:#F4B400,stroke:#000,color:#000
    style COMP fill:#F4B400,stroke:#000,color:#000
    style BQ_EXT fill:#0F9D58,stroke:#000,color:#fff
    style BQ_VIEW fill:#0F9D58,stroke:#000,color:#fff
    style LOOKER fill:#0F9D58,stroke:#000,color:#fff
    style TF fill:#623CE4,stroke:#000,color:#fff
    style CB fill:#466FBC,stroke:#000,color:#fff
    style GIT fill:#f5f5f5,stroke:#333,color:#333
```

**Flux :**
1.  Cloud Scheduler d√©clenche une Cloud Function toutes les 10 minutes.
2.  La Cloud Function appelle l'API OpenSky Network, r√©cup√®re les √©tats des vols pour la zone d√©finie, et √©crit les donn√©es JSON brutes dans un bucket GCS (Landing Zone), partitionn√© par date et heure d'ingestion.
3.  Cloud Composer ex√©cute un DAG Airflow (ex: toutes les heures).
4.  Le DAG soumet un job PySpark √† un cluster Dataproc.
5.  Le job Spark lit les donn√©es JSON brutes de l'heure pr√©c√©dente depuis la Landing Zone GCS.
6.  Spark nettoie, transforme, structure les donn√©es (extrait les champs, convertit les types) et ajoute des colonnes de partition (ann√©e, mois, jour, heure).
7.  Spark √©crit le r√©sultat au format Parquet dans un second bucket GCS (Processed Zone), en utilisant le partitionnement Hive.
8.  Une table externe dans BigQuery est d√©finie pour pointer vers les donn√©es Parquet partitionn√©es sur GCS.
9.  Des vues BigQuery sont cr√©√©es par-dessus la table externe pour simplifier les requ√™tes et pr√©-calculer des KPIs.
10. Les donn√©es sont accessibles via SQL dans BigQuery ou via des outils de BI comme Looker Studio connect√©s aux vues/tables.
11. L'infrastructure est g√©r√©e par Terraform (IaC).
12. Cloud Build assure l'int√©gration continue (linting, tests) et potentiellement le d√©ploiement continu (non impl√©ment√© compl√®tement dans cette phase).

## üõ†Ô∏è Technologies Utilis√©es

* **Cloud Platform :** Google Cloud Platform (GCP)
* **Services GCP Principaux :**
    * Cloud Functions
    * Cloud Storage (GCS)
    * Dataproc (pour Spark)
    * BigQuery (Data Warehouse + SQL Engine)
    * Cloud Composer (v2 - Apache Airflow manag√©)
    * Cloud Scheduler
    * Cloud Build (CI/CD)
    * IAM (Identity and Access Management)
    * Looker Studio (Visualisation)
    * *(Optionnel: Secret Manager, Artifact Registry)*
* **Langages :** Python 3.11, SQL (GoogleSQL), Bash
* **Frameworks / Biblioth√®ques :** PySpark, Apache Airflow, Pandas (potentiellement), Requests, google-cloud-python libraries, Flask (via Functions Framework)
* **Infrastructure & CI/CD :** Terraform, Git, Docker (implicitement via Cloud Build/Functions)
* **Formats de Donn√©es :** JSON (brut), Parquet (trait√©)

## üìä Donn√©es Source

Les donn√©es proviennent de l'[API REST de OpenSky Network](https://openskynetwork.github.io/opensky-api/rest.html). Elles consistent en des "vecteurs d'√©tat" (state vectors) d'a√©ronefs transmis via ADS-B et d'autres syst√®mes, contenant des informations telles que :
* Identifiant `icao24`
* Indicatif d'appel (`callsign`)
* Position (`longitude`, `latitude`)
* Altitude (`baro_altitude`, `geo_altitude`)
* Vitesse (`velocity`)
* Cap (`true_track`)
* Taux de mont√©e/descente (`vertical_rate`)
* Statut au sol (`on_ground`)
* Timestamps (`time_position`, `last_contact`)
* Code transpondeur (`squawk`)
* Pays d'origine (`origin_country`)

L'API publique est utilis√©e, avec un filtrage g√©ographique appliqu√© lors de l'ingestion pour se concentrer sur la r√©gion d'int√©r√™t.

## ‚öôÔ∏è Installation et Ex√©cution (Instructions Haut Niveau)

Ce projet n√©cessite une configuration sp√©cifiquesur GCP.

**Pr√©requis :**
* Compte Google Cloud avec facturation activ√©e (attention aux co√ªts de Composer et Dataproc si actifs).
* `gcloud` CLI install√© et configur√© (`gcloud auth login`, `gcloud auth application-default login`, `gcloud config set project ...`).
* `terraform` CLI install√©.
* `git` install√©.
* Python 3.11 (ou compatible) et `pip` pour l'environnement virtuel local.
* Java JDK (ex: 11) install√© localement avec `JAVA_HOME` configur√© (pour les tests Spark locaux).

**√âtapes :**
1.  **Cloner le D√©p√¥t :** `git clone https://github.com/remigarcia31/toulouse-aero-analysis.git`
2.  **Environnement Local :** `cd toulouse-aero-analysis` puis `python -m venv venv` et `source venv/bin/activate`. Installez les d√©pendances : `pip install -r src/cloud_function_ingest/requirements.txt -r requirements.txt`.
**TODO : FAIRE UN .env.example**
3.  **Configuration :** Cr√©ez un fichier `.env` √† la racine en vous basant sur un √©ventuel `.env.example` (assurez-vous que `.env` est dans `.gitignore`). Remplissez les variables (ID projet, noms de buckets...).
4.  **Infrastructure :** Naviguez dans `terraform/`, lancez `terraform init` puis `terraform apply`. Cela cr√©era les buckets GCS, le dataset BigQuery, les comptes de service et les permissions IAM.
5.  **Composants Applicatifs (Exemple D√©ploiement Initial):**
    * **Cloud Function :** D√©ployez via `gcloud functions deploy ...` (voir les commandes dans les √©tapes pr√©c√©dentes du guide).
    * **Script Spark :** Uploadez sur GCS (`gsutil cp src/spark_job/... gs://...`).
    * **DAG Airflow :** Uploadez sur le bucket GCS de Composer (`gsutil cp dags/... gs://...`).
    * *(Note: Id√©alement, ces d√©ploiements seraient g√©r√©s par l'√©tape CD de Cloud Build).*
6.  **Orchestration & Scheduler :**
    * Cr√©ez l'environnement Cloud Composer (via `gcloud composer environments create ...` - **Attention aux co√ªts !**).
    * Cr√©ez le job Cloud Scheduler (via `gcloud scheduler jobs create ...`) pour d√©clencher la fonction d'ingestion. Mettez-le en pause initialement si vous faites un backfill manuel.
7.  **Backfill Historique (Optionnel - Manuel) :**
    * Cr√©ez un cluster Dataproc temporaire (`gcloud dataproc clusters create ...`).
    * Utilisez le script `backfill_spark_jobs.sh` (apr√®s l'avoir configur√© via `.env`) pour soumettre les jobs Spark pour chaque heure historique.
    * **SUPPRIMEZ** le cluster Dataproc apr√®s le backfill (`gcloud dataproc clusters delete ...`).
8.  **Lancement du Pipeline Orchestr√© :**
    * Assurez-vous que le cluster Dataproc manuel (`aero-cluster-test`) est supprim√© (le DAG le r√©f√©rence encore, il faudra le modifier pour utiliser des clusters √©ph√©m√®res).
    * Activez ("Unpause") le DAG `aero_data_processing_pipeline` dans l'UI Airflow.
    * Activez ("Resume") le job Cloud Scheduler (`trigger-opensky-ingest-workaround`).
    * Le pipeline devrait maintenant tourner de mani√®re autonome (Scheduler -> Function -> GCS Landing -> Composer/Airflow -> Dataproc -> GCS Processed -> BQ External Table).

## üìä R√©sultats & Visualisation

Les donn√©es trait√©es et partitionn√©es sont disponibles pour analyse via SQL dans BigQuery en utilisant la table `toulouse-aero-analysis.aeronautics_data.flight_data_external` ou les vues `vw_*` associ√©es.

Un tableau de bord Looker Studio a √©t√© cr√©√© pour explorer interactivement ces donn√©es. Il inclut :
* Une carte de la position des avions.
* L'√©volution du nombre d'avions uniques par heure.
* La r√©partition du trafic par pays d'origine.
* Des indicateurs sur l'altitude et la vitesse moyennes.
* Des filtres par date et par pays.

** TODO : Ins√©rez ici une capture d'√©cran dashboard Looker Studio)**
`![Aper√ßu Dashboard Looker Studio](chemin/vers/screenshot_dashboard.png)`

** TODO : Optionnel : ajoutez un lien dashboard public)**
`[Voir le Dashboard Interactif](LIEN_LOOKER_STUDIO_PUBLIC)`

## üöÄ Am√©liorations Possibles

* **CI/CD Compl√®te :** Automatiser le d√©ploiement de toutes les ressources (Terraform apply, Cloud Function, DAG, script Spark) via Cloud Build.
* **Clusters Dataproc √âph√©m√®res :** Modifier le DAG Airflow pour cr√©er un cluster Dataproc √† la demande pour chaque job Spark et le supprimer ensuite (`DataprocCreateClusterOperator`, `DataprocDeleteClusterOperator`). C'est plus √©conomique.
* **Monitoring & Alerting :** Mettre en place des alertes Cloud Monitoring sur les √©checs de DAG, les erreurs de fonction, ou des m√©triques m√©tier (ex: volume de donn√©es anormal). Int√©grer les `sla_miss_callback` d'Airflow.
* **Data Quality Checks :** Ajouter une √©tape dans le DAG (apr√®s Spark) pour valider la qualit√© des donn√©es √©crites (ex: via BigQueryOperator lan√ßant des requ√™tes SQL de validation, ou int√©gration d'outils comme Great Expectations ou dbt).
* **Enrichissement des Donn√©es :** Joindre les donn√©es ADS-B avec des bases de donn√©es externes (via `icao24`) pour obtenir le type d'avion, la compagnie a√©rienne, l'√¢ge de l'appareil, etc. et permettre des analyses plus riches.
* **Streaming R√©el :** Remplacer l'ingestion par batch (Cloud Function toutes les 10 min) par un pipeline streaming avec Pub/Sub et Dataflow pour une latence plus faible.
* **Tests :** √âtoffer les tests unitaires et ajouter des tests d'int√©gration.
* **S√©curit√© :** Affiner les permissions IAM au plus juste besoin pour chaque compte de service.

## üë§ Auteur

* **R√©mi GARCIA**
* **LinkedIn :** `https://www.linkedin.com/in/remi-garcia-31t12r/`
* **GitHub :** `https://github.com/remigarcia31`
